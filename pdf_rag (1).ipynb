{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hG4S1-3uYtPw"
      },
      "outputs": [],
      "source": [
        "!pip install pypdf chromadb google-generativeai langchain-community\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U google-genai\n"
      ],
      "metadata": {
        "id": "9ZkkAYvfa7sM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.genai import Client\n",
        "from google.genai.types import Content, Part\n"
      ],
      "metadata": {
        "id": "WYymsG1ca_wA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------------------------------------\n",
        "# üîë SECURITY STEP: SAFE API KEY HANDLING (Run this first)\n",
        "# -------------------------------------------------------------------------\n",
        "import os\n",
        "import google.generativeai as genai\n",
        "from google.genai import Client # This handles the 'client = Client()' part\n",
        "from google.colab import userdata\n",
        "\n",
        "# 1. Fetch the key safely\n",
        "try:\n",
        "    # Try to load from Colab Secrets\n",
        "    SECRET_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "except Exception:\n",
        "    # If not found (e.g., for GitHub users), ask for input\n",
        "    print(\"‚ö†Ô∏è Key not found in Secrets.\")\n",
        "    SECRET_KEY = input(\"Please enter your Google Gemini API Key: \")\n",
        "\n",
        "# 2. Configure the FIRST library (google.generativeai)\n",
        "genai.configure(api_key=SECRET_KEY)\n",
        "\n",
        "# 3. Configure the SECOND library (google.genai)\n",
        "client = Client(api_key=SECRET_KEY)\n",
        "\n",
        "print(\"‚úÖ Security Check: API Key configured for both libraries!\")"
      ],
      "metadata": {
        "id": "T-Y4Fvmmuxvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pypdf import PdfReader\n",
        "\n",
        "def extract_text_from_pdf(file):\n",
        "    reader = PdfReader(file)\n",
        "    text = \"\"\n",
        "    for page in reader.pages:\n",
        "        text += page.extract_text() or \"\"\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "lXTychJNZD7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "def chunk_text(text, chunk_size=800, chunk_overlap=150):\n",
        "    splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap\n",
        "    )\n",
        "    return splitter.split_text(text)\n"
      ],
      "metadata": {
        "id": "Dn3NK_NDZHb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embedding_new(text):\n",
        "    # Use genai.embed_content directly, which is part of google.generativeai\n",
        "    res = genai.embed_content(\n",
        "        model=\"models/text-embedding-004\", # Model name for embedding\n",
        "        content=text\n",
        "    )\n",
        "    return res['embedding']"
      ],
      "metadata": {
        "id": "Ir2XG7oYZMVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import chromadb\n",
        "from chromadb.config import Settings\n",
        "\n",
        "# Initialize an in-memory ChromaDB client for no persistence.\n",
        "# The default client is in-memory DuckDB.\n",
        "chroma_client = chromadb.Client()\n",
        "\n",
        "collection = chroma_client.get_or_create_collection(\"pdf_rag\")"
      ],
      "metadata": {
        "id": "nRJjyrwDZQyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "file_name = next(iter(uploaded))\n",
        "\n",
        "pdf_text = extract_text_from_pdf(file_name)\n",
        "chunks = chunk_text(pdf_text)\n",
        "embeddings = embed_chunks(chunks)\n",
        "\n",
        "# add to Chroma\n",
        "ids = [str(i) for i in range(len(chunks))]\n",
        "collection.add(documents=chunks, embeddings=embeddings, ids=ids)\n",
        "\n",
        "len(chunks)\n"
      ],
      "metadata": {
        "id": "eN6P5PbkZUYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def answer_question(question, top_k=3):\n",
        "    # 1) embed question\n",
        "    q_emb = get_embedding_new(question)\n",
        "\n",
        "    # 2) retrieve chunks\n",
        "    results = collection.query(\n",
        "        query_embeddings=[q_emb],\n",
        "        n_results=top_k\n",
        "    )\n",
        "\n",
        "    retrieved_docs = results[\"documents\"][0]\n",
        "    context = \"\\n\\n\".join(retrieved_docs)\n",
        "\n",
        "    # 3) build prompt\n",
        "    prompt = f\"\"\"\n",
        "    Answer the following question using ONLY the context below.\n",
        "\n",
        "    CONTEXT:\n",
        "    {context}\n",
        "\n",
        "    QUESTION:\n",
        "    {question}\n",
        "\n",
        "    ANSWER:\n",
        "    \"\"\"\n",
        "\n",
        "    # 4) generate answer\n",
        "    model = genai.GenerativeModel('models/gemini-pro-latest') # Changed model name to 'models/gemini-pro-latest'\n",
        "    response = model.generate_content(\n",
        "        contents=prompt\n",
        "    )\n",
        "\n",
        "    return response.text"
      ],
      "metadata": {
        "id": "KWuFwMZPZvNK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Listing available models:')\n",
        "for m in genai.list_models():\n",
        "  if 'generateContent' in m.supported_generation_methods:\n",
        "    print(m.name)"
      ],
      "metadata": {
        "id": "ZCRW4e0dZyzz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2bd2dc0"
      },
      "source": [
        "question = 'what skills does this candidate have?'\n",
        "answer = answer_question(question)\n",
        "print(answer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = input(\"Ask something: \")\n",
        "print(answer_question(question))\n"
      ],
      "metadata": {
        "id": "oEBAxt-7fcu-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio\n"
      ],
      "metadata": {
        "id": "dTeYT1tRf7N4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import time\n",
        "import google.generativeai as genai\n",
        "import chromadb\n",
        "from pypdf import PdfReader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# --- 1. SETUP & CONFIGURATION ---\n",
        "# (Assumes you have already run genai.configure(api_key=...) in previous cells)\n",
        "\n",
        "# Initialize ChromaDB\n",
        "chroma_client = chromadb.Client()\n",
        "COLLECTION_NAME = \"gradio_pdf_rag\"\n",
        "\n",
        "# Ensure clean start\n",
        "try:\n",
        "    chroma_client.delete_collection(COLLECTION_NAME)\n",
        "except:\n",
        "    pass\n",
        "collection = chroma_client.create_collection(COLLECTION_NAME)\n",
        "\n",
        "# --- 2. REAL BACKEND FUNCTIONS (Adapted from your Notebook) ---\n",
        "\n",
        "def extract_text_from_pdf_real(file_path):\n",
        "    reader = PdfReader(file_path)\n",
        "    text = \"\"\n",
        "    for page in reader.pages:\n",
        "        text += page.extract_text() or \"\"\n",
        "    return text\n",
        "\n",
        "def chunk_text_real(text, chunk_size=800, chunk_overlap=150):\n",
        "    splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap\n",
        "    )\n",
        "    return splitter.split_text(text)\n",
        "\n",
        "def get_embedding_real(text):\n",
        "    # Using your specific model from the notebook\n",
        "    res = genai.embed_content(\n",
        "        model=\"models/text-embedding-004\",\n",
        "        content=text\n",
        "    )\n",
        "    return res['embedding']\n",
        "\n",
        "def answer_question_real(question, top_k=5):\n",
        "    # 1. Embed the user's question\n",
        "    q_emb = get_embedding_real(question)\n",
        "\n",
        "    # 2. Query ChromaDB\n",
        "    results = collection.query(\n",
        "        query_embeddings=[q_emb],\n",
        "        n_results=top_k\n",
        "    )\n",
        "\n",
        "    # Check if we found anything\n",
        "    if not results['documents'] or not results['documents'][0]:\n",
        "        return \"I couldn't find any relevant information in the PDF.\"\n",
        "\n",
        "    # 3. Prepare Context\n",
        "    retrieved_docs = results[\"documents\"][0]\n",
        "    context = \"\\n\\n\".join(retrieved_docs)\n",
        "\n",
        "    # 4. Construct Prompt\n",
        "    prompt = f\"\"\"\n",
        "    Answer the following question using ONLY the context provided below.\n",
        "    If the answer is not in the context, state that you don't know.\n",
        "\n",
        "    CONTEXT:\n",
        "    {context}\n",
        "\n",
        "    QUESTION:\n",
        "    {question}\n",
        "\n",
        "    ANSWER:\n",
        "    \"\"\"\n",
        "\n",
        "    # 5. Generate Answer with Gemini\n",
        "    model = genai.GenerativeModel('models/gemini-pro-latest')\n",
        "    response = model.generate_content(prompt)\n",
        "    return response.text\n",
        "\n",
        "# --- 3. GRADIO LOGIC (Connecting UI to Backend) ---\n",
        "\n",
        "def process_pdf_ui(file):\n",
        "    if file is None:\n",
        "        return False, gr.Info(\"‚ö†Ô∏è Please upload a PDF file first.\")\n",
        "\n",
        "    try:\n",
        "        # Reset collection for new file\n",
        "        global collection\n",
        "        try:\n",
        "            chroma_client.delete_collection(COLLECTION_NAME)\n",
        "        except:\n",
        "            pass\n",
        "        collection = chroma_client.create_collection(COLLECTION_NAME)\n",
        "\n",
        "        # 1. Extract\n",
        "        text = extract_text_from_pdf_real(file.name)\n",
        "\n",
        "        # 2. Chunk\n",
        "        chunks = chunk_text_real(text)\n",
        "\n",
        "        # 3. Embed & Store (Process loop)\n",
        "        # Note: In production, batching is better, but this works for valid notebook use\n",
        "        embeddings = []\n",
        "        ids = []\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            emb = get_embedding_real(chunk)\n",
        "            embeddings.append(emb)\n",
        "            ids.append(str(i))\n",
        "\n",
        "        collection.add(documents=chunks, embeddings=embeddings, ids=ids)\n",
        "\n",
        "        return True, gr.Info(\"‚úÖ PDF Processed Successfully! Ask away.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        return False, gr.Info(f\"‚ùå Error: {str(e)}\")\n",
        "\n",
        "def chat_ui(message, history, is_processed):\n",
        "    if not is_processed:\n",
        "        return \"‚ö†Ô∏è Please upload and process a PDF using the sidebar first.\"\n",
        "\n",
        "    # Call the real answer function\n",
        "    return answer_question_real(message)\n",
        "\n",
        "# --- 4. UI LAYOUT ---\n",
        "theme = gr.themes.Soft(\n",
        "    primary_hue=\"blue\",\n",
        "    secondary_hue=\"slate\",\n",
        "    text_size=\"lg\"\n",
        ")\n",
        "\n",
        "with gr.Blocks(theme=theme, title=\"Gemini RAG Analyst\") as demo:\n",
        "\n",
        "    # State to track if PDF is ready\n",
        "    pdf_state = gr.State(False)\n",
        "\n",
        "    with gr.Row():\n",
        "        # --- Sidebar ---\n",
        "        with gr.Column(scale=1, variant=\"panel\"):\n",
        "            gr.Markdown(\"## üìÇ Document Hub\")\n",
        "            file_input = gr.File(label=\"Upload PDF\", file_types=[\".pdf\"])\n",
        "            process_btn = gr.Button(\"üöÄ Process PDF\", variant=\"primary\")\n",
        "\n",
        "            # Invisible element to catch output updates\n",
        "            status_txt = gr.Markdown(visible=False)\n",
        "\n",
        "            gr.Markdown(\"---\")\n",
        "            gr.Markdown(\"**Instructions:**\\n1. Upload PDF\\n2. Click Process\\n3. Wait for Success\\n4. Chat\")\n",
        "\n",
        "        # --- Chat Area ---\n",
        "        with gr.Column(scale=3):\n",
        "            gr.Markdown(\"## ü§ñ AI Research Assistant\")\n",
        "\n",
        "            chatbot = gr.Chatbot(height=600, type=\"messages\", show_copy_button=True)\n",
        "\n",
        "            chat_int = gr.ChatInterface(\n",
        "                fn=chat_ui,\n",
        "                chatbot=chatbot,\n",
        "                additional_inputs=[pdf_state],\n",
        "                textbox=gr.Textbox(placeholder=\"Ask a question about the uploaded PDF...\"),\n",
        "                theme=\"soft\"\n",
        "            )\n",
        "\n",
        "    # Event Listener\n",
        "    process_btn.click(\n",
        "        fn=process_pdf_ui,\n",
        "        inputs=[file_input],\n",
        "        outputs=[pdf_state, status_txt]\n",
        "    )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch(debug=True)"
      ],
      "metadata": {
        "id": "dIbngiwTf999"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}